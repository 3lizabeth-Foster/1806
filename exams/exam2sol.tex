\documentclass[12pt]{article}
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}

\title{18.06 Exam 2 Solutions}
\date{Johnson, Spring 2022}

\begin{document}

\maketitle

\begin{enumerate}[1.]
    \item To fit the given points $(x_k,y_k,z_k) \in \{(1,2,7),(0,0,2),(-1,0,3),(1,1,4),(2,-1,5)\}$, we have
    \[
    \left\{\begin{array}{c}
    \alpha x_1 + \beta y_1 + \gamma = z_1,\\
    \alpha x_2 + \beta y_2 + \gamma = z_2,\\
    \alpha x_3 + \beta y_3 + \gamma = z_3,\\
    \alpha x_4 + \beta y_4 + \gamma = z_4,\\
    \alpha x_5 + \beta y_5 + \gamma = z_5.
    \end{array}\right.
    \]
    Writing the above as a matrix equation, we have
    \[
    \begin{pmatrix} x_1 & y_1 & 1 \\
    x_2 & y_2 & 1 \\
    x_3 & y_3 & 1 \\
    x_4 & y_4 & 1 \\
    x_5 & y_5 & 1 
    \end{pmatrix} \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix} = \begin{pmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \\ z_5 \end{pmatrix}.
    \]
    In other words, we have
    \[ A x = b\]
    where
    \[
    \boxed{A =  \begin{pmatrix} 1 & 2 & 1 \\
    0 & 0 & 1 \\
    -1 & 0 & 1 \\
    1 & 1 & 1 \\
    2 & -1 & 1 
    \end{pmatrix}}, \ \ \boxed{b = \begin{pmatrix} 7 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}}.
    \]
    But of course, this is overdetermined (more equations than unknowns) and is unlikely to have an exact solution.
    Instead, the problem requests the least-square solution, corresponding to minimizing $\Vert b - Ax \Vert^2$, which
    yields the normal equations:
    \[ \boxed{A^T A \hat{x} = A^T b} ,\]
    where $\hat{x} = (\hat{\alpha}, \hat{\beta}, \hat{\gamma})$ are the best-fit parameters.
    Writing this out explicitly by plugging in the numbers (which was \emph{not} required) yields:
    \[
    \boxed{\begin{pmatrix} 
    1 & 0 & -1 & 1 & 2 \\
    2 & 0 & 0 & 1 & -1\\
    1 & 1 & 1 & 1 & 1
    \end{pmatrix} 
    \begin{pmatrix} 
    1 & 2 & 1 \\
    0 & 0 & 1 \\
    -1 & 0 & 1 \\
    1 & 1 & 1 \\
    2 & -1 & 1 
    \end{pmatrix} 
    \begin{pmatrix} \hat{\alpha} \\ \hat{\beta} \\ \hat{\gamma} \end{pmatrix} = \begin{pmatrix} 1 & 0 & -1 & 1 & 2 \\
    2& 0 & 0 & 1 & -1\\
    1 & 1 & 1 & 1 & 1
    \end{pmatrix} \begin{pmatrix} 7 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}}.
    \]
    \hfill $\blacksquare$
    
    \newpage
    \item \begin{enumerate}[(a)]
        \item As $b \in C(A) = C(Q)$, we can write $b$ as
        \[
        b = QQ^T b = q_1 (q_1^T b) +   q_2 (q_2^T b) + q_3 (q_3^T b) =  \boxed{3\sqrt{2} q_1 -4q_2 + 8 q_3} \, ,
        \]
        recalling that the coefficients of an orthonormal basis are obtained merely by dot products (i.e. projections $qq^T$).
        
        \item Since $N(A^T) = C(A)^{\perp}$, we can get the orthogonal projection of $y = \begin{pmatrix} 2 \\ -2 \\ 2 \\ -2 \end{pmatrix}$ onto $N(A^T)$ by simply subtracting the projection of $y$ onto the $q$'s. In other words, the orthogonal projection of $y$ onto $N(A^T)$ is
        \begin{align*}
        (I- QQ^T) y &= y -  q_1 (q_1^T y) -  q_2(q_2^T y) -q_3 (q_3^T y)  \\
        &=  \begin{pmatrix} 2 \\ -2 \\ 2 \\ -2 \end{pmatrix} -0\begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}} \end{pmatrix} - 2\begin{pmatrix} \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ -\frac{1}{2} \end{pmatrix} - 2\begin{pmatrix} \frac{1}{2} \\ -\frac{1}{2} \\ -\frac{1}{2} \\ -\frac{1}{2} \end{pmatrix} = \boxed{\begin{pmatrix} 0 \\ -2 \\ 2 \\ 0 \end{pmatrix}}.
        \end{align*}
        %Since $N(A^T) = C(A)^{\perp}$, the dimension of $N(A^T)$ is $4-3 = 1$. Note that $a = \begin{pmatrix} 0 \\ 1 \\ -1 \\ 0 \end{pmatrix}$ is a vector orthogonal to $q_1, q_2, q_3$ and hence $N(A^T) = \text{span}\{a\}$. Therefore, the orthogonal projection of $y = \begin{pmatrix} 2 \\ -2 \\ 2 \\ -2 \end{pmatrix}$ onto $N(A^T)$ is
        %\[\frac{a a^T}{a^T a}y = a \frac{a^T y}{a^T a} =  \begin{pmatrix} 0 \\ 1 \\ -1 \\ 0 \end{pmatrix} \frac{-2-2}{1+1} = \boxed{\begin{pmatrix} 0 \\ -2 \\ 2 \\ 0 \end{pmatrix}}.\]
        
        \item The terms $\boxed{q_2^T a_1,  q_3^T a_1, q_3^T a_2}$ must be 0. \\
        \\
        In general, for $A = \begin{pmatrix} a_1 & a_2 & ... & a_n \end{pmatrix}$ with linearly independent columns, the QR factorization obtained using Gram--Schmidt is $$A = QR,$$ where $Q = \begin{pmatrix} q_1 & q_2 & ... & q_n \end{pmatrix}$ is a $m\times n$ matrix with orthonormal columns spanning $C(A)$ and $R = \begin{pmatrix} r_{11} & r_{21} & \dots & r_{n1} \\ 0 & r_{22} & \dots & r_{n2} \\ & & \vdots & \\ 0 & 0 & \dots & r_{nn}\end{pmatrix}$ is an $n \times n$ invertible upper-triangular matrix, with $r_{ij} = q_i^T a_j$ for all $i \geq j$.\\
        \\
        Another way of seeing the same thing is to recall the Gram--Schmidt process.  By construction, $q_1$ is parallel to $a_1$, so $q_2$ and $q_3$ must be $\perp a_1$.  $a_2$ is in the span of $q_1$ and $q_2$, so we must also have $q_3 \perp a_2$.  \hfill $\blacksquare$
    \end{enumerate}
    
    \pagebreak
    \item For $f(x) = (b - Ax)^T M (b-Ax)$, recall from class that $d(y^T M y) = dy^T M y + y^T M dy = 2 dy^T M y$ (using $M = M^T$). For $y = b - Ax$, we have $dy = -Adx$. Combining these equations yields:
    \[
    df = 2 dy^T M y = 2 (-Adx)^T M (b - Ax) = dx^T \underbrace{[-2 A^T M A (b - Ax)]}_{\nabla f} \, ,
    \]
    since the gradient is defined by $df = \nabla f ^T dx = dx^T \nabla f$.  Alternatively, going through all of the steps explicitly using the product rule, we have
    \[
    \begin{split}
    df &= d((b - Ax)^T M (b-Ax)) \\
    &= (d(b - Ax)^T) M (b-Ax) + (b - Ax)^T (dM) (b-Ax) + (b - Ax)^T M (d(b-Ax)) \\
    &= -(A dx)^T M (b-Ax) + 0 - (b - Ax)^T M A dx \ \ \text{ (since $dA, db, dM$ all vanish)}\\
    &=-(M(b-Ax))^T (A dx) - (b-Ax)^T M A dx \ \ \text{ (since $x^T y = y^T x$ for column vectors $x,y$)}\\
    &=-((b-Ax)^T M^T A + (b-Ax)^T MA) dx\\
    &=-2(b-Ax)^T M^T A dx \ \ \ \ \ \  \text{ (since $M^T = M$)}\\
    &= {\underbrace{(-2A^T M (b-Ax))}_{\nabla f}}^T dx.
    \end{split}
    \]
    Therefore, when $\nabla f = 0$, we have
    \[-2A^T M (b-Ax) = 0 \Longleftrightarrow \boxed{A^T M A x = A^T Mb}.\]
        \hfill $\blacksquare$
        \newpage
    \item
    \begin{enumerate}[(a)]
        \item  If $A = \begin{pmatrix} a_1 & a_2 \end{pmatrix}$, the projection matrix onto $C(A)$ is given by $\frac{a_1 a_1^T}{a_1^T a_1} + \frac{a_2 a_2^T}{a_2^T a_2}$ only when $\boxed{a_1, a_2 \text{ are orthogonal}}$ ($\ne$ orthonormal).\\
        In general, we have $P = A (A^T A)^{-1} A^T = \begin{pmatrix} a_1 & a_2 \end{pmatrix} \begin{pmatrix} a_1^T a_1 & a_1^T a_2 \\ a_2^T a_1 & a_2^T a_2 \end{pmatrix}^{-1}  \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}$, which would have terms involving both $a_1$ and $a_2$ if they are not orthogonal.
        \item If $S$ and $T$ are orthogonal subspaces of a vector space $V$, then
        \begin{enumerate}[(i)]
            \item their intersection (vectors in both $S$ and $T$) is the set $\boxed{\{\vec{0}\}}$. \\Note that if $x \in S \cap T$ then $x^T x = 0 \Rightarrow x = 0$.
            
            \item (dimension of $S$) + (dimension of $T$) must be $\boxed{\leq}$ (dimension of $V$).\\
            \\
            (The sum $= \mbox{dimension }V$ only when $S$ and $T$ are \emph{orthogonal complements}, not merely orthogonal.)
            For example, $S = \text{span}\left\{\begin{pmatrix}
            1\\0\\0
            \end{pmatrix}\right\}$ and $T = \text{span}\left\{\begin{pmatrix}
            0\\1\\0
            \end{pmatrix}\right\}$ are two orthogonal subspaces of $V = \mathbb{R}^3$, and we have (dimension of $S$) + (dimension of $T$) $= 1+1 = 2 \leq 3$.
            
        \end{enumerate}
        
        \item For the vector space $\mathbb{R}^3$, give projection matrices onto:
        \begin{enumerate}[(i)]
            \item any 0-dimensional subspace: $\boxed{P = \begin{pmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\end{pmatrix}}$, i.e. the $3 \times 3$ zero matrix.
            (Note that the only 0-dimensional subspace is $\{\vec{0}\}$.)
            \item any 1-dimensional subspace: $\boxed{P = \frac{a a^T}{a^T a}}$ for $S = \text{span}\{a\}$ with some $a \ne \vec{0}$. A specific example is $\boxed{P = \begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\end{pmatrix}}$ for $S = \text{span}\left\{\begin{pmatrix}
            1\\0\\0
            \end{pmatrix}\right\}$.
            \item any 3-dimensional subspace: $\boxed{P = I_3 = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{pmatrix} }$, i.e. the $3 \times 3$ identity matrix. Note that the only subpsace of $\mathbb{R}^3$ with dimension 3 is $\mathbb{R}^3$ itself. %If $C(A)$ is a 3-dimensional subspace, then $P = A(A^T A)^{-1} A^T = A A^{-1} (A^T)^{-1} A^T = I$. 
        \end{enumerate}
        \item We must have $Q^T Q = I$ for orthonormal columns, but $\boxed{QQ^T \ne I}$ is possible whenever $Q$ is not square (not unitary), in which case $QQ^T$ is the projection matrix onto a lower-dimensional subspace $C(Q)$ of the whole space.   In particular, you just need any ``tall'' $Q$ matrix: orthonormal columns, but fewer columns than rows, such as the $Q$ matrix of problem~2.  \\
        \\
        The simplest example is a $Q$ matrix with only a \emph{single} orthonormal column, in which $QQ^T$ is projection onto a 1d subspace, such as:
        \[\boxed{Q = \begin{pmatrix} 1 \\ 0 \end{pmatrix}}, \ \
        QQ^T = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \neq I.\]
        % \[\boxed{Q = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix}}, \ \
        % QQ^T = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix} \neq I\]
        
        \item $A$ is a $7\times 5$ matrix of rank 4.
        \begin{enumerate}[(i)]
            \item Give the size and rank of the following projection matrices:
            \begin{enumerate}[i.]
                \item $P_1$ = projection onto $C(A)$: $\boxed{\text{size} = 7\times 7, \text{ rank } = 4}$
                \item $P_2$ = projection onto $C(A^T)$: $\boxed{\text{size} = 5\times 5, \text{ rank } = 4}$
                \item $P_3$ = projection onto $N(A)$: $\boxed{\text{size} = 5\times 5, \text{ rank } = 5-4 = 1}$
                \item $P_4$ = projection onto $N(A^T)$: $\boxed{\text{size} = 7\times 7, \text{ rank } = 7-4 = 3}$ 
            \end{enumerate}
            \item Give a sum or product of two of these $P$ matrices that must $= 0$ (a zero matrix):
            Note that $\boxed{P_1 P_4 = 0}$ as $C(A)$ and $N(A^T)$ are orthogonal complements. Similarly, we have $\boxed{P_4 P_1 = 0}$, $\boxed{P_2 P_3 = 0}$, $\boxed{P_3 P_2 = 0}$.
            \item Give a sum or product of two of these $P$ matrices that must = $I$ (an identity matrix):
            As $C(A)$ and $N(A^T)$ are orthogonal complements, we have $P_4 = I - P_1$. Therefore, $\boxed{P_1 + P_4 = I}$. Similarly, $\boxed{P_2 + P_3 = I}$.
        \hfill $\blacksquare$
        \end{enumerate}

    \end{enumerate}
   
\end{enumerate}

\end{document}
