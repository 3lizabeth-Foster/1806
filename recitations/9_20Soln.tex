\documentclass{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts, dsfont, fancyhdr, graphicx, color, tabularx, enumitem}
\usepackage{geometry}


\theoremstyle{definition}
\newtheorem{prob}{}
\renewcommand{\qedsymbol}{}
\renewcommand*{\proofname}{Solution}
\newcommand{\MSB}[1]{\textcolor{blue}{[MSB: #1]}}

\pagestyle{fancy} \fancyhf{} \lhead{\textsc{18.06}} \rhead{9/20/22} 

\begin{document}


\section*{Practice Problems}

\begin{prob}
	Say $P$ is a permutation. Why is $P^{-1}=P^T$? (You're allowed to use this fact in the pset without justification, but it's good to know why it's true! If you get stuck, answer this question for a particular permutation.)
	
	\vspace{12pt}

\noindent\textbf{Bonus question:} Say $P[x_1, \dots, x_n]=[x_{p_1}, \dots, x_{p_n}]$ and $P^{-1}[x_1, \dots, x_n]=[x_{q_1}, \dots, x_{q_n}]$. What's the relationship between the lists $(p_1, \dots, p_n)$ and $(q_1, \dots, q_n)$?

\end{prob}

\begin{proof}
Say $P[x_1, \dots, x_n]=[x_{p_1}, \dots, x_{p_n}]$. So if we write $P$ as a matrix, the entry in row $1$, col $p_1$ is 1 and all other entries in row $1$ are 0; the entry in row $2$, col $p_2$ is 1 and all other entries in row $2$ are 0; and so on. The matrix $P^T$ is obtained by ``reflecting $P$ across the diagonal". So the entry in row $r$, col $c$ of $P$ is in row $c$, col $r$ of $P^T$.
	
	The permutation $P^{-1}$ should undo $P$, so $P^{-1}[x_{p_1}, \dots, x_{p_n}]= [x_1, \dots, x_n]$. That is, multiplying by $P^{-1}$ moves $x_{p_1}$ from row $1$ to row $p_1$ of the column vector, moves $x_{p_2}$ from row $2$ to row $p_2$ of the column vector, and so on. This means that, writing $P^{-1}$ as a matrix, the entry in row $p_1$, col $1$ is 1 and all other entries in row $p_1$ are zero; the entry in row $p_2$, col $2$ is 1 and all other entries in that row are 0; and so on. This is exactly the definition of $P^T$.
	
	For the bonus question: Remember that $PP^{-1}=I$. So if you apply $P$ to $[x_{q_1}, \dots, x_{q_n}]$, you must get $[x_1, \dots, x_n]$. On the other hand, if you apply $P$ to any vector $y$, you move the entry in row $p_1$ to row $1$. So the entry in row $p_1$ of $[x_{q_1}, \dots, x_{q_n}]$ must be $x_1$. That is, $p_1$ is the position of $1$ in the list $(q_1, \dots, q_n)$. The same argument shows that $p_i$ is the position of the number $i$ in the list $(q_1, \dots, q_n)$; similarly, $q_i$ is the position of the number $i$ in the list $(p_1, \dots, p_n)$.
\end{proof}

\begin{prob}
	Say 
	\[A=\begin{bmatrix}
		1& 3& 0& 1\\
		0&1&0&2\\
		0&0&1&1\\
		0&0&0&1
	\end{bmatrix}
\begin{bmatrix}
	-1& 0& 0& 0\\
	0&2&0&0\\
	0&0&-3&0\\
	0&0&0&-2
\end{bmatrix}
\begin{bmatrix}
	0& 0& 0& 1\\
	0&0&1&-1\\
	0&1&0&3\\
	1&1&0&1
\end{bmatrix}.\]
Solve 
\[Ax=\begin{bmatrix}
	12\\6\\32\\2
\end{bmatrix}\]
for $x$ \emph{without} using Gaussian elimination.
\end{prob}

\begin{proof}
	This is similar to (and actually a little easier than) Problem 5 from last week. Say the matrices in the product are $B, C, D$, so $A=BCD$. We want to solve 
	\[BCDx=\begin{bmatrix}
		12\\6\\32\\2
	\end{bmatrix}.\]
Because $A$ is already factored into matrices with very special shapes, we will turn this into a sequence of problems, each of which we'll solve using an easy substitution. First, set $y=CDx$, so we have 
	\[By=\begin{bmatrix}
		1& 3& 0& 1\\
		0&1&0&2\\
		0&0&1&1\\
		0&0&0&1
	\end{bmatrix} \begin{bmatrix}
	y_1\\y_2\\y_3\\y_4
\end{bmatrix}=\begin{bmatrix}
	12\\6\\32\\2
\end{bmatrix}.\]
Multiplying $B$ and $y$ gives a triangular system of equations, which we solve using back substitution to get 
\[y_4=2 \quad y_3=30 \quad y_2=2 \quad y_1=4.\]

Now that we have $y$, we want to solve $CDx=y$. There are two options: set $Dx=z$ and solve $Cz=y$, then use the $z$ you find to solve $Dx=z$. Or do the matrix multiplication $CD$ to get some new matrix $F$ and solve $Fx=y$. I'll do the second, since multiplying by a diagonal matrix is very easy.
\[CDx=\begin{bmatrix}
	0& 0& 0& -1\\
	0&0&2&-2\\
	0&-3&0&9\\
	-2&-2&0&-2
\end{bmatrix}\begin{bmatrix}
x_1\\x_2\\x_3\\x_4
\end{bmatrix}= \begin{bmatrix}
4\\2\\30\\2
\end{bmatrix}\]
where the rightmost vector is $y$. Using subsitution, we get
\[x_4=-4 \quad x_3=-3 \quad x_2=2 \quad x_1=1\]
so $x=[1,2,-3,-4].$
\end{proof}

\begin{prob}
	Consider the following \emph{tridiagonal} matrix
	\[A=\begin{bmatrix}
		1 & -1& 0&0&0\\
		2&-1&1&0&0\\
		0&3&4&-2&0\\
		0&0&-2&5&-2\\
		0&0&0&-1&3\\
	\end{bmatrix}.\]
\begin{itemize}
	\item[a)] Compute its $LU$ factorization using Gaussian elimination. What do you notice about the pattern of nonzero entries?
	\item[b)] Compute the 3rd column of $A^{-1}$. What do you notice about the pattern of nonzero entries?
	\item[c)] Suppose you carried out arithmetic at the same rate, but $A$ was a $250 \times 250$ tridiagonal matrix instead of $5 \times 5$. How much longer would a) have taken?
\end{itemize}
\end{prob}
\begin{proof}
For a): We do Gaussian elimination to compute $U$ and remember the multipliers for the row operations (these will give the off-diagonal entries of $L$ once we swap the signs).
	\[\begin{bmatrix}
		1 & -1& 0&0&0\\
		2&-1&1&0&0\\
		0&3&4&-2&0\\
		0&0&-2&5&-2\\
		0&0&0&-1&3\\
	\end{bmatrix} \xrightarrow{r_2-2r_1}
\begin{bmatrix}
	1 & -1& 0&0&0\\
	0&1&1&0&0\\
	0&3&4&-2&0\\
	0&0&-2&5&-2\\
	0&0&0&-1&3\\
\end{bmatrix} \xrightarrow{r_3-3r_2}
\begin{bmatrix}
1 & -1& 0&0&0\\
0&1&1&0&0\\
0&0&1&-2&0\\
0&0&-2&5&-2\\
0&0&0&-1&3\\
\end{bmatrix} \]\[\xrightarrow{r_4+2r_3}
\begin{bmatrix}
1 & -1& 0&0&0\\
0&1&1&0&0\\
0&0&1&-2&0\\
0&0&0&1&-2\\
0&0&0&-1&3\\
\end{bmatrix}\xrightarrow{r_5+r_4}
\begin{bmatrix}
1 & -1& 0&0&0\\
0&1&1&0&0\\
0&0&1&-2&0\\
0&0&0&1&-2\\
0&0&0&0&1\\
\end{bmatrix}=U.\]
The matrix $L$ is lower triangular with 1's on the diagonal. The entries below the diagonal are the negatives of the multipliers we used above:
\[
L=\begin{bmatrix}
	1 & 0& 0&0&0\\
	2&1&0&0&0\\
	0&3&1&0&0\\
	0&0&-2&1&0\\
	0&0&0&-1&1\\
\end{bmatrix}.\]
$U$ and $L$ are \emph{bidiagonal} matrices.

For b): Finding the third column of $A^{-1}$ is solving the equation 
\[Ax=\begin{bmatrix}
	0\\0\\1\\0\\0
\end{bmatrix}.\]
The easiest way to do this is to use the $A=LU$ factorization from a) and break the problem down into intermediate, easier problems. Set $y=Ux$, so we have 
\[Ly=\begin{bmatrix}
	1 & 0& 0&0&0\\
	2&1&0&0&0\\
	0&3&1&0&0\\
	0&0&-2&1&0\\
	0&0&0&-1&1\\
\end{bmatrix}\begin{bmatrix}
y_1\\y_2\\y_3\\y_4\\y_5
\end{bmatrix}= \begin{bmatrix}
0\\0\\1\\0\\0
\end{bmatrix}.\] Substitution gives 
\[\begin{bmatrix}
	y_1\\y_2\\y_3\\y_4\\y_5
\end{bmatrix}= \begin{bmatrix}
	0\\0\\1\\ 2\\2
\end{bmatrix}.\]
Now we solve $Ux=y$ for $x$. That is, we want $x$ so that
\[\begin{bmatrix}1 & -1& 0&0&0\\
0&1&1&0&0\\
0&0&1&-2&0\\
0&0&0&1&-2\\
0&0&0&0&1\\
\end{bmatrix}\begin{bmatrix}
x_1\\x_2\\x_3\\x_4\\x_5
\end{bmatrix}= \begin{bmatrix}
0\\0\\1\\ 2\\2
\end{bmatrix}.\]
Substitution gives 
\[\begin{bmatrix}
	x_1\\x_2\\x_3\\x_4\\x_5
\end{bmatrix}= \begin{bmatrix}
-13\\-13\\13	\\6\\2
\end{bmatrix}.\]
All entries of the third column of $A^{-1}$ are nonzero (and in fact, all other entries of $A^{-1}$ are non-zero too). The conclusion you should draw is that this is another reason it's better to use $LU$ factorization to solve $Ax=b$ than it is to invert $A$. The $L, U$ matrices were even more sparse than $A$, while $A^{-1}$ is not sparse at all.

For c): Each step of the Gaussian elimination for $A$ involved only 4 operations: multiplying the two nonzero entries of the row by the multiplier, then subtracting those entries from the next row. So the cost of Gaussian elimination for tridiagonal matrices is roughly linear in the number of rows. This means we expect a) to take 50 times as long for a matrix which has 50 times more rows than $A$.
\end{proof}

\begin{prob} Are the following subsets of $\mathbb{R}^2$ vector spaces? Why or why not?
	\begin{itemize}
		\item [a)] $[x, y]$ satisfying $mx+b=y$, where $m, b$ are fixed scalars.
		\item [b)] $[x,y]$ satisfying $x^2 + y^2 =1$ (the unit circle).
		\item [c)] $[x, y]$ satisfying 
		\[A \begin{bmatrix}
			x \\y
		\end{bmatrix}=0\]
	where $A$ is a fixed $2 \times 2$ matrix.
	\end{itemize}
\end{prob}

\begin{proof}
	\begin{itemize}
		\item[a)] If $b \neq 0$, then the computation from Problem 1 of the last worksheet shows that adding two vectors in this subset doesn't produce another vector this subset. Any sum of elements in a vector space should still be in the vector space, so this is not a vector space. If $b=0$, then Problem 1 from the last worksheet shows that adding two vectors in this subset, or scaling a vector in this subset, gives another vector in this subset, so it is a vector space.
		\item[b)] Not a vector space---scaling a vector on the unit circle does not give a vector on the unit circle.
		\item[c)] This is always a vector space. If $A[x, y]=0$ and $A[u, w]=0$, then $A[x+u, y+w]=0$ and $A[cx, cy]=0$.
	\end{itemize}
\end{proof}


\end{document}