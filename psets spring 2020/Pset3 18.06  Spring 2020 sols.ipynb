{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pset 3 (spring 2020) <br>\n",
    "(Optional: if you wish to download this notebook as an .ipynb file, use the download button in the upper right and OPTION-click (MAC) or ALT-Click (Linux and Windows ) then \"Save Link as..\" or \"Download Linked File As..\" or something similar on your browser.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1. Let x̄$\\ = \\frac{1}{n}\\sum_{i=1}^n x_i$ denote the mean of x. What is \n",
    "sum( x.-x̄).  (Remember x.-x̄ is the vector x with an elementwise subtraction of x̄). Explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to try some examples in Julia (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.220446049250313e-16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "n = 8\n",
    "x = rand(n)\n",
    "x̄ = mean(x)\n",
    "sum( x .- x̄)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1. The sum is zero, as it is equal to $\\sum_{i=1}^n(x_i-x̄)=\\sum_{i=1}^n(x_i)-nx̄=n\\sum_{i=1}^n(\\frac{x_i}{n})-nx̄=nx̄-nx̄=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.  (We may supply some hints as we go, so check back in ) <br>\n",
    "This problem shows how the QR idea can derive the formulas for best fit lines. <br>\n",
    "a)Suppose (xᵢ,yᵢ) for i=1,...m are data which we will fit with a best least squares line.  Let\n",
    "a = x .- x̄ and b = y .- ȳ. <br> The slope of the best line through the (aᵢ,bᵢ) is 1) the same  2) bigger 3) smaller\n",
    "than the line through the (xᵢ,yᵢ) for i=1,...m ?  Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In terms of a and b, write an m x 2 matrix A  and a right hand side expressing $A [slope; intercept] = $ right hand side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the dot product of the first columns of A with the second?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the result in c to derive the QR factorization of $A$ without too much hard work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What is the slope of the best least squares line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) The intercept is very simple?  Why is this result obvious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) What are the slope and intercept for the original data (xᵢ,yᵢ) without any hard work?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2. a) 1) The same. After the translation of $(-x̄,-ȳ)$, the best fit line for $(x_i,y_i)$ coincide with that for $(a_i,b_i)$. This is because parallel translation preserves the distance between points and lines, which implies the error function of points with respect to a line is the same as that after translation. \n",
    "\n",
    "b) $\\begin{pmatrix} a_1 & 1 \\\\ a_2 & 1 \\\\ \\vdots & \\vdots \\\\ a_m & 1 \\end{pmatrix}\\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix}+\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_m \\end{pmatrix}$\n",
    "\n",
    "c) $0$, by Problem 1. \n",
    "\n",
    "d) $\\begin{pmatrix} a_1 & 1 \\\\ a_2 & 1 \\\\ \\vdots & \\vdots \\\\ a_m & 1 \\end{pmatrix}=\\begin{pmatrix} a_1 / \\sqrt{\\sum_{i=1}^ma_i^2} & 1/ \\sqrt{m} \\\\ a_2\\sqrt{\\sum_{i=1}^ma_i^2} & 1/ \\sqrt{m} \\\\ \\vdots & \\vdots \\\\ a_m\\sqrt{\\sum_{i=1}^ma_i^2} & 1/ \\sqrt{m} \\end{pmatrix}\\begin{pmatrix} \\sqrt{\\sum_{i=1}^ma_i^2} & 0 \\\\ 0 & \\sqrt{m} \\end{pmatrix}$\n",
    "\n",
    "e) We have to minimize the norm of $\\vec{b}-A \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}$. By Pythagorean theorem, $A \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}$ has to be the projection of $\\vec{b}$ to the column space of $A$. In other words, $A \\vec{y} \\cdot \\left( \\vec{b}-A \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix} \\right) =0$ for any $y$. Since the left side is $\\vec{y} ^TA^T \\left( \\vec{b}-A \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix} \\right) $, we know that $A^T \\vec{b}=A^TA \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}$, which is equal to $(QR)^T(QR)\\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}=R^TQ^TQR \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}= R^TR \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}= \\begin{pmatrix}\\sum_{i=1}^ma_i^2 & 0 \\\\ 0 & m \\end{pmatrix} \\begin{pmatrix} slope \\\\ intercept \\end{pmatrix}=\\begin{pmatrix} (\\sum_{i=1}^ma_i^2) \\times slope \\\\ m \\times intercept \\end{pmatrix}$. By comparing the first entry of both sides, we have $\\sum_{i=1}^ma_ib_i= (\\sum_{i=1}^ma_i^2) \\times slope$, i.e. $slope= \\frac{\\sum_{i=1}^ma_ib_i}{\\sum_{i=1}^ma_i^2}$.\n",
    "\n",
    "f) Compare the second entry instead of first entry in part (e), we know $\\sum_{i=1}^mb_i= m \\times intercept$. By Problem 1, $\\sum_{i=1}^mb_i=0$. Hence $intercept =0$. It is believable as the \"center of mass\" of $(a_i,b_i)$ is the origin. \n",
    "\n",
    "g) Part (a) and (e) tell us that the slope is $\\frac{\\sum_{i=1}^ma_ib_i}{\\sum_{i=1}^ma_i^2}$. Moreover the best fit line has to pass through $(x̄,ȳ)$, the \"center of mass\" of original data. The intercept has to be $ȳ-\\frac{\\sum_{i=1}^ma_ib_i}{\\sum_{i=1}^ma_i^2}x̄$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.  Suppose A=QR is square, where Q is orthogonal and R is upper triangular and invertible. Write the solution to Ax=b in terms of b and possibly Q,Qᵀ,and R.\n",
    "\n",
    "Solution 3. $x=R^{-1}Q^Tb$ because $x=A^{-1}b=(QR)^{-1}b=R^{-1}Q^{-1}b=R^{-1}Q^Tb$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4. Set up a matrix least squares problem if we are interested in taking n data points $(x_i,y_i)$ and we wish to find the best function f(x)=$c_1e^x+c_2e^{−x}$  through the data points.\n",
    "\n",
    "Solution 4. Denote $Y=(y_1, y_2, ... , y_n)^T, E^1=(e^{x_1}, e^{x_2}, ... , e^{x_n})^T, E^2=(e^{-x_1}, e^{-x_2}, ... , e^{-x_n})^T$. Then the error term $\\epsilon$ is the $Y-\\begin{pmatrix} E^1 & E^2 \\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$, hence the quantity we minimize is $\\epsilon^T\\epsilon$. We take its partial derivative with respect to $c_k$ for $k=1,2$ and set them to be zero. $0=\\frac{\\partial}{\\partial c_k}(\\epsilon^T\\epsilon)= \\frac{\\partial}{\\partial c_k}(\\epsilon^T)\\epsilon+\\epsilon^T\\frac{\\partial}{\\partial c_k}(\\epsilon)=-(E^k)^T\\epsilon- \\epsilon^TE^k=-2(E^k)^T\\epsilon$. The second equality is product rule and the last equality holds because transpose of a real number is itself. Substitute $\\epsilon=Y-\\begin{pmatrix} E^1 & E^2 \\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$, we get $(E^k)^T\\begin{pmatrix} E^1 & E^2 \\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}=(E^k)^TY$ for $k=1,2$. In other words, $\\begin{pmatrix} (E^1)^T \\\\ (E^2)^T \\end{pmatrix}\\begin{pmatrix} E^1 & E^2 \\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}=\\begin{pmatrix} (E^1)^T \\\\ (E^2)^T \\end{pmatrix}Y$. Rewrite it in terms of $x_i$, $y_i$, the matrix form of least squares is $\\begin{pmatrix} \\sum e^{2x_i} & n \\\\ n &  \\sum e^{-2x_i} \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}=\\begin{pmatrix} \\sum y_ie^{x_i} \\\\ \\sum y_ie^{-x_i} \\end{pmatrix}$, where the summation is from $1$ to $n$.\n",
    "\n",
    "\n",
    "Remark: If you don't believe taking partial derivatives in matrices is legal, here is an explanation from Calculus perspective: We want to minimize $\\sum_i(y_i-c_1e^{x_i}-c_2e^{-x_i})^2$. Call this function $g$. It suffices to solve equations $\\frac{\\partial g}{\\partial c_1}=0$, $\\frac{\\partial g}{\\partial c_2}=0$. By direct computation, $\\frac{\\partial g}{\\partial c_1}=-2\\sum y_ie^{x_i}+2c_2n+2c_1\\sum e^{2x_i}$ and $\\frac{\\partial g}{\\partial c_2}=-2\\sum y_ie^{-x_i}+2c_1n+2c_2\\sum e^{-2x_i}$. Hence the matrix form of least squares is $\\begin{pmatrix} 2\\sum e^{2x_i} & 2n \\\\ 2n & 2 \\sum e^{-2x_i} \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}=\\begin{pmatrix} 2\\sum y_ie^{x_i} \\\\ 2\\sum y_ie^{-x_i} \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5. Find $A^T$ and $A^{-1}$ and $(A^{-1})^T$ and $(A^T)^{-1}$ for\n",
    "$A=\\left( \\begin{matrix} 1 & 0 \\\\ 9 & 3 \\end{matrix}\\right)$.\n",
    "\n",
    "Solution 5. $A^T=\\begin{pmatrix} 1 & 9 \\\\ 0 & 3\\end{pmatrix}$ , $A^{-1}= \\begin{pmatrix} 1 & 0 \\\\ -3 & 1/3 \\end{pmatrix}$, $(A^{-1})^T= (A^T)^{-1} = \\begin{pmatrix} 1 & -3 \\\\ 0 & 1/3 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6. A matrix $A$ is symmetric if $A=A^T$.  Which of these are true?  <br>\n",
    "The block matrix $\\begin{pmatrix} 0  & A\\\\ A  & 0 \\end{pmatrix}$ is automatically symmetric. <br>\n",
    "If A and B are symmetric then their product AB is symmetric. <br>\n",
    "If A is not symmetric then $A^{−1}$ is not symmetric. <br>\n",
    "When A,B,C are symmetric, the transpose of ABC is CBA <br>\n",
    "If $A=A^T$ and $B=B^T$, which of these matrices are certainly symmetric? <br>\n",
    "\n",
    "1. $A^2−B^2$ \n",
    "2. $(A+B)(A−B)$\n",
    "3. $ABA$\n",
    "4.  $ABAB$\n",
    "\n",
    "Solution 6. \n",
    "Only \"If A is not symmetric then $A^{−1}$ is not symmetric\" and \"When A,B,C are symmetric, the transpose of ABC is CBA\" are true. \n",
    "1) $A^2−B^2$ and 3)$ABA$ must be symmetric. Taking $A=\\begin{pmatrix} 1  & 1\\\\ 1  & 0 \\end{pmatrix}$ and $B= \\begin{pmatrix} 1  & 1\\\\ 1  & 1 \\end{pmatrix}$ provides non-symmetric examples in 2) and 4). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 (GS p.106 13)\n",
    "Compute U and L for the symmetric matrix A:  <br>\n",
    "$\\left( \\begin{matrix}\n",
    "a & a  & a & a \\\\\n",
    "a & b  & b & b \\\\\n",
    "a & b  & c & c \\\\\n",
    "a & b  & c &  d\n",
    "\\end{matrix} \\right) $.\n",
    "Find four conditions on a,b,c,d to get A=LU with four pivots.\n",
    "\n",
    "Solution 7. $\\begin{pmatrix}\n",
    "a & a  & a & a \\\\\n",
    "a & b  & b & b \\\\\n",
    "a & b  & c & c \\\\\n",
    "a & b  & c & d \\end{pmatrix}=\\begin{pmatrix}\n",
    "1 & 0  & 0 & 0 \\\\\n",
    "1 & 1  & 0 & 0 \\\\\n",
    "1 & 0  & 1 & 0 \\\\\n",
    "1 & 0  & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "1 & 0  & 0 & 0 \\\\\n",
    "0 & 1  & 0 & 0 \\\\\n",
    "0 & 1  & 1 & 0 \\\\\n",
    "0 & 1  & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "1 & 0  & 0 & 0 \\\\\n",
    "0 & 1  & 0 & 0 \\\\\n",
    "0 & 0  & 1 & 0 \\\\\n",
    "0 & 0  & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "a & a  & a & a \\\\\n",
    "0 & b-a  & b-a & b-a \\\\\n",
    "0 & 0  & c-b & c-b \\\\\n",
    "0 & 0  & 0 & d-c\n",
    "\\end{pmatrix}$. Hence $\\begin{pmatrix}\n",
    "a & a  & a & a \\\\\n",
    "a & b  & b & b \\\\\n",
    "a & b  & c & c \\\\\n",
    "a & b  & c & d \\end{pmatrix}=\\begin{pmatrix}\n",
    "1 & 0  & 0 & 0 \\\\\n",
    "1 & 1  & 0 & 0 \\\\\n",
    "1 & 1  & 1 & 0 \\\\\n",
    "1 & 1  & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "a & a  & a & a \\\\\n",
    "0 & b-a  & b-a & b-a \\\\\n",
    "0 & 0  & c-b & c-b \\\\\n",
    "0 & 0  & 0 & d-c\n",
    "\\end{pmatrix}$ is the LU factorization. $A$ has four pivot if and only if $a,b-a,c-b,d-c$ are non-zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8.  First computation of a singular value decomposition. <br>\n",
    "\n",
    "\n",
    "Here we wish for you to become familiar with an SVD by changing the m and n from what appears here\n",
    "and testing that the results have the required properties.  You merely need to execute and turn in\n",
    "a screen shot. Describe in a few words what kind of matrix is U, V, and diagm(Σ) for your chosen m and n.\n",
    "Say sizes, and use words like orthogonal, tall skinny orthogonal, diagonal, triangular etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One version of the SVD produces $$ A = U * Diagonal(\\Sigma) * V',$$ where  $U'U=I$, $V'V=I$, $\\Sigma=(\\sigma_1,\\sigma_2,...)$ with $\\sigma_1 \\ge \\sigma_2 \\ge ... \\gt 0$.  <br>\n",
    "$U$ is $m \\times n$ and $V$ is $n \\times n$ if $ m \\ge n$, and \n",
    "$U$ is $m \\times m$ and $V$ is $n \\times m$ if $ m \\lt n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 2.0908607204660132\n",
       " 0.5438073613767322\n",
       " 0.1985541583756679"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "A = rand(5,3)\n",
    "U,Σ,V = svd(A)\n",
    "display(Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U'U ≈ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V'V ≈ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A ≈ U * diagm(Σ) * V'\n",
    "A ≈ U * Diagonal(Σ) * V'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 8. The following answer depends on the choice of $m$ and $n$. \n",
    "First case is when $m > n$, (e.g. $A$ is a $5 \\times 3$ matrix as mentioned above). $U$ is a tall skinny orthogonal matrix of size $m \\times n$; $diagm(\\Sigma)$ is a diagonal matrix of size $n \\times n$ and $V$ is an orthogonal matrix of size $n \\times n$. \n",
    "\n",
    "Second case is when $m < n$. $U$ is an orthogonal matrix of size $m \\times m$; $diagm(\\Sigma)$ is a diagonal matrix of size $m \\times m$ and $V$ is a tall skinny orthogonal matrix of size $n \\times m$.\n",
    "\n",
    "Third case is when $m=n$. $U$ is an orthogonal matrix of size $m \\times m$; $diagm(\\Sigma)$ is a diagonal matrix of size $m \\times m$ and $V$ is an orthogonal matrix of size $m \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 9. Suppose a square $A$ has an LU factorization $A=LU$ where $L$ and $U$ are invertible. If $A=QR$, what is $r_{11}$ in terms of possibly elements of $L$ and $U$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 9. $r_{11}=\\sqrt{\\sum_i U_{11}^2L_{i1}^2}$. We know that $r_{11}$ is the norm of first column vector of $A$, which is equal to $U_{11}$ times the first column vector of $L$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 10 .  Suppose an n x 2 matrix $A$ is written as $QR$, where $Q$ is a tall-skinny orthogonal matrix and is also n x 2 and R = $ \\left( \\begin{matrix} 1 & 3 \\\\ 0 & 4 \\end{matrix} \\right)  $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the norm of the second column of $A$? Briefly explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 10. Its norm is $5$. Call the $i$-th column vector of $A$ be $A_i$ and the $i$-th column vector of $Q$ be $Q_i$. By orthogonality of $Q$, we know that $Q_1 \\cdot Q_1=1$, $Q_1 \\cdot Q_2=1$, $Q_1 \\cdot Q_2=0$. $A_2=3Q_1+4Q_2$ implies its norm is $\\sqrt{3^2+4^2}=5$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
