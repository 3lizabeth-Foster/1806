\documentclass{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts, dsfont, fancyhdr, graphicx, color, tabularx, enumitem}
\usepackage{geometry}


\theoremstyle{definition}
\newtheorem{prob}{}
\renewcommand{\qedsymbol}{}
\renewcommand*{\proofname}{Solution}
\newcommand{\MSB}[1]{\textcolor{blue}{[MSB: #1]}}

\pagestyle{fancy} \fancyhf{} \lhead{\textsc{18.06}} \rhead{9/13/22} 

\begin{document}


\section*{Solutions}
\begin{prob}[Lecture recap---skip if you feel like it] A function $f$ is called ``linear" if $f(x+y)=f(x) + f(y)$ for all $x$ and $y$ and $f(rx)=r f(x)$ for any scalar $r$.
	\begin{itemize}
		\item [a)] Is $f(x)=m x +b$ linear? What about $f(x)=x^2$? (In both cases, $f$ is a function on the real numbers)
		\item [b)] Show that $f(x)=Ax$ is linear for any $2 \times 2$ matrix $A$. (Here, $x$ is any $2 \times 1$ vector.)
		\item [c)] Show that $f(X)=AX$ is linear for any $2 \times 2$ matrix $A$. (Here, $X$ is any $2 \times 2$ matrix.)
	\end{itemize}
\end{prob}

\begin{proof}\ 
	\begin{itemize}
		\item [a)] For the first function, we check 
		\begin{align*}
			f(x+y)&= mx +b + m y + b\\
			&= m(x+y) + 2b
		\end{align*}
	while $f(x+y)=m(x+y)+b$. These are equal only if $b=0$, so $f$ is not linear if $b \neq 0$.
	If $b=0$, then $f(rx)=rx=rf(x)$, so $f$ is linear. 
	
	For the second function, $f(rx)=r^2 x^2 \neq r f(x)$. So $f$ is not linear.
	\item [b)] Write 
	\[A=\begin{bmatrix}
		a & b\\
		c & d
	\end{bmatrix}, x=\begin{bmatrix}
	x_1\\x_2
\end{bmatrix}, y=\begin{bmatrix}
y_1\\y_2
\end{bmatrix}.\]
Then 
\[Ax=\begin{bmatrix}
	a x_1 + b x_2 \\c x_1 + dx_2
\end{bmatrix}, Ay=\begin{bmatrix}
a y_1 + b y_2 \\c y_1 + dy_2
\end{bmatrix}, A(x+y)=\begin{bmatrix}
a (x_1+y_1 )+ b (x_2+ y_2) \\c (x_1 + y_1) + d(x_2+y_2)
\end{bmatrix}\] and 
\[A (rx)= \begin{bmatrix}
	a r x_1 + b r x_2 \\c r x_1 + d r x_2
\end{bmatrix}= r \begin{bmatrix}
a x_1 + b x_2 \\c x_1 + dx_2
\end{bmatrix} = rAx.\]
So $f$ is linear.
\item [c)] Write $X=[x ~ y]$ and $V=[u~ v]$; that is, $x, y, u, v$ are 2-component column vectors. Then 
\[AX + AV=[Ax ~ Ay]+[Au~Av]=[Ax+Au ~ Ay+ Av] \]
and 
\[ A(X + V)=[A(x+u)~A(y+v)].\]
Using b), we know that these are equal. Also 
\[A(rX)=[Arx~Ary]=[rAx~ rAy]=rAx,\]
again using b).
	\end{itemize}
\end{proof}

\begin{prob}
	Say $x, y, z$ are $4$-component column vectors. The equation
	$$x(y+z)=xy + xz = yx + zx$$
	is nonsense (why?) but is a few symbols away from being true. Decorate with transposes to make it a true equation.
\end{prob}
\begin{proof}
	The equation is nonsense because we can't multipl two 4-component vectors. One way to decorate with transposes to get a true equation is
	\[x^T(y+z)=x^Ty + x^T z=y^T x+ z^T x.\]
\end{proof}
 

\begin{prob}
Say $P$ is the $4 \times 4$ linear operation that reverses the order, i.e.
\[P \begin{bmatrix}
	x_1 \\ x_2 \\ x_3\\ x_4
\end{bmatrix}= \begin{bmatrix}
x_4 \\ x_3 \\ x_2\\ x_1
\end{bmatrix}.\]
What does $P$ do to the $4 \times 4$ identity matrix $I$? How can you use this to write down $P$?

More generally, if you know how a linear operation $A$ behaves on a vector of variables, how can you write down the matrix for $A$?
\end{prob}
\begin{proof}
	\[P\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0& 1 & 0 & 0\\
		0& 0 & 1& 0\\
		0 & 0 & 0 & 1\\
	\end{bmatrix}= \begin{bmatrix}
	0 & 0 & 0 & 1\\
	0& 0 & 1 & 0\\
	0& 1 & 0& 0\\
	1 & 0 & 0 & 0\\
\end{bmatrix}.\]

Remember that the identity matrix satisfies $IP=PI=P$, so the matrix $PI$ above is exactly $P$. In general, you can compute the matrix for any linear operation by writing down how it acts on the columns (or rows) of $I$.
\end{proof}

\begin{prob}
	Find the $LU$ factorization of 
	\[A=\begin{bmatrix}
		a & a & a\\
		a & b & b\\
		a & b& c
	\end{bmatrix}.\]
What 3 conditions on $a, b, c$ guarantee that $A=LU$ has 3 pivots?
\end{prob}
\begin{proof}
	We start by using Gaussian elimination to find $U$. Subtracting the first row from the second and third gives
	\[\begin{bmatrix}
		a & a & a\\
		0 & b-a & b-a\\
		0 & b-a& c-a
	\end{bmatrix}.\]
Subtracting the second row from the third gives 
	\[\begin{bmatrix}
	a & a & a\\
	0 & b-a & b-a\\
	0 & 0& c-b
\end{bmatrix}=U.\]
To compute $L$, we remember that the elimination matrices are 
\[E_1=	\begin{bmatrix}
	1 & 0 & 0\\
	-1 & 1& 0\\
	-1& 0& 1
\end{bmatrix}\text{ and }E_2=	\begin{bmatrix}
	1 & 0 & 0\\
	0& 1& 0\\
	0& -1& 1
\end{bmatrix}\]
so $E_2 E_1 A= U$. The elimination matrices are nonsingular and square, so $A=E_1^{-1} E_2^{-1} U$. We have 
\[E_1^{-1}=	\begin{bmatrix}
	1 & 0 & 0\\
	1 & 1& 0\\
	1& 0& 1
\end{bmatrix}, \quad
E_2^{-1}=	\begin{bmatrix}
	1 & 0 & 0\\
	0& 1& 0\\
	0& 1& 1
\end{bmatrix}.\]
So $L= E_1^{-1} E_2^{-1}$, which is
\[\begin{bmatrix}
	1 & 0 & 0\\
	1& 1& 0\\
	1& 1& 1
\end{bmatrix}.\]
(You should learn a faster way to compute $L$ in one of the next lectures.)

To guarantee $A$ has 3 pivots, we need $a \neq 0$, $a \neq b$ and $b \neq c$.
\end{proof}

\begin{prob}
	Consider the matrices
	\[U=\begin{bmatrix}
		1 & 1 & -1\\
		0 & 1 & 2\\
		0 & 0 & 1
	\end{bmatrix}, 
L=\begin{bmatrix}
	1 & 0& 0\\
	-1 & 1 & 0\\
	-2 & 1& 1
\end{bmatrix}, 
B=\begin{bmatrix}
	1 & 2 & 3\\
	3 & 2 & 1\\
	1 & 0 & 1
\end{bmatrix} \]
and set $A=U B^{-1}L$.
Without inverting any matrices, compute the second column of $A^{-1}$.
\end{prob}
\begin{proof}
	The second column $x$ of $A^{-1}$ is 
	\[x=A^{-1}\begin{bmatrix}
		0 \\ 1\\0
	\end{bmatrix}.\] That is, $x$ is the vector so that $Ax=[0~1~0]^T$. Since $A=U B^{-1}L$, we are actually trying to solve
\[U B^{-1}L x =\begin{bmatrix}
	0 \\ 1\\0
\end{bmatrix}.\]
We'll do this in a couple of steps. Let $y= B^{-1}L x$, so we need to solve
\[U y =\begin{bmatrix}
0 \\ 1\\0
\end{bmatrix}.\]
Since $U$ is upper triangular, we can do this just by substitution: 
\begin{alignat*}{3}
	y_3&=0 &\implies y_3 &=0\\
	y_2 + 2 y_3&=1& \implies y_2&=1\\
		y_1+y_2 +  y_3&=0& \implies y_1&=-1.\\
\end{alignat*}
Now we want to solve $B^{-1}Lx=y$ for $x$. This is the same as solving $Lx=By$ for $x$. Remember, we know $y$, so the right hand side is just a vector. So we have 
\[\begin{bmatrix}
	1 & 0& 0\\
	-1 & 1 & 0\\
	-2 & 1& 1
\end{bmatrix} \begin{bmatrix}
x_1\\x_2\\x3
\end{bmatrix}= \begin{bmatrix}
1 & 2 & 3\\
3 & 2 & 1\\
1 & 0 & 1
\end{bmatrix}\begin{bmatrix}
-1\\1\\0
\end{bmatrix} = \begin{bmatrix}
1 \\-1\\-1
\end{bmatrix}.\]
We can finish using substitution again:
\begin{alignat*}{3}
	x_1&=1 &\implies x_1 &=1\\
	-x_1 + x_2&=-1& \implies x_2&=0\\
	-2 x_1 + x_2 + x_3&=-1& \implies x_3&=1.\\
\end{alignat*}
\end{proof}



\end{document}