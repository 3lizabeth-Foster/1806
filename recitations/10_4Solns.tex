\documentclass{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts, dsfont, fancyhdr, graphicx, color, tabularx, enumitem}
\usepackage{geometry}


\theoremstyle{definition}
\newtheorem{prob}{}
\renewcommand{\qedsymbol}{}
\renewcommand*{\proofname}{Solution}
\newcommand{\MSB}[1]{\textcolor{blue}{[MSB: #1]}}

\pagestyle{fancy} \fancyhf{} \lhead{\textsc{18.06}} \rhead{10/4/22} 

\begin{document}


\section*{Practice Problems}
\begin{prob} True or False (give a good reason if true/counterexample or reason if false)

\begin{enumerate}
	
	\item If the zero vector is in the column space of a matrix $A$, then the columns of $A$ are linearly dependent.\\
	\textbf{Solution:} False; $A=I$ is a counterexample. The zero vector is in the column span of every matrix, because the zero vector is in every subspace.
	
	\item If the columns of a matrix are dependent, so are the rows.
	\\
	\textbf{Solution:} False. A counterexample is any matrix with more columns than rows, but full row rank, e.g.
	\[\begin{pmatrix} 1 & 0 & 1\\ 0 & 1 & 1 \end{pmatrix}. \]
	
	\item The column space of a $2 \times 2$ matrix is the same as its row space.\\
	\textbf{Solution:} False. Consider 
	\[A = \begin{pmatrix} 1 & 0 \\ 1 & 0 \end{pmatrix}. \]
Then $C(A) = \left\langle \begin{pmatrix}1 \\ 1 \end{pmatrix} \right\rangle$, but $R(A) =  \left\langle \begin{pmatrix}1 \\ 0 \end{pmatrix} \right\rangle$.
	
	
	\item The column space of a $2 \times 2$ matrix has the same dimension as its row space.\\
	\textbf{Solution:} True. The dimensions of both spaces are the rank of $A$.
	
	\item The columns of a matrix are a basis for the column space.\\
	\textbf{Solution:} False. The columns will always span the column space, but they may not be linearly independent. A counterexample is any matrix with a column of all 0's, or any matrix with more columns than rows.
	
	\item $A$ and $A^T$ have the same number of pivots.\\ \textbf{Solution:} True. The number of (nonzero) pivots is the rank of $A$, which is equal to the rank of $A^T$.
	
	\item $A$ and $A^T$ have the same left nullspace. \\ \textbf{Solution:} False. The left nullspace of $A$ is $N(A^T)$. The left nullspace of $A^T$ is $N(A)$. These are usually not equal; for example, if $A$ is $2 \times 3$, then $N(A)$ is a subspace of $\mathbb{R}^3$ and $N(A^T)$ is a subspace of $\mathbb{R}^2$.
	
	\item If the row space equals the column space then $A^T=A$. \\ \textbf{Solution:} False. A counterexample is any invertible matrix which is not symmetric, like 
	\[A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}. \].
	
	\item If $A^T = -A$, then the row space of $A$ equals the column space. \\ \textbf{Solution:} True. $C(A) = C(-A) = C(A^T) = R(A)$.
	
\end{enumerate}
	
\end{prob}


\begin{prob} If $w_1, w_2, w_3$ are independent vectors in $\mathbb{R}^3$, show that the differences 
\begin{align*}
	v_1 &= w_2 - w_3 \\
	v_2 &= w_1 - w_3 \\
	v_3 &= w_1 - w_2.
\end{align*}
are \emph{dependent}.  Find the matrix $A$ so that
\[ \begin{bmatrix} \\ w_1 & w_2 & w_3 \\ &\end{bmatrix}A = \begin{bmatrix} \\ v_1 & v_2 & v_3 \\ &\end{bmatrix}.\]
Which matrices above are singular?
\end{prob}

\begin{proof}
	To show that $v_1, v_2, v_3$ are dependent, we need to find a linear relation that they satisfy. Playing around, you can see that 
\[v_1 - v_2 + v_3 = (w_2-w_3) - (w_1-w_3) + (w_1 - w_2) = 0.\]
The matrix $A$ is
\[A = \begin{pmatrix} 0 & 1 & 1 \\ 1 & 0 & -1 \\ -1 & -1 & 0 \end{pmatrix}. \]
The matrix $(v_1 \ v_2 \ v_3)$ is singular, and so is $A$ (if $A$ weren't singular, then it would be impossible for $(v_1 \ v_2 \ v_3)$ to be singular).
\end{proof}


\begin{prob}Construct $A = uv^T + wz^T$ whose column space has basis $\begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}, \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ and whose row space has basis $(1,0), (1,1)$.  Write $A$ as a $3 \times 2$ matrix times a $2 \times 2$ matrix.
\end{prob}

\begin{proof} From the problem, we know $A$ should be $3 \times 2$.
	Let's try to find $A$ such that the columns are an invertible linear combination of $u = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}$, and $w=\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$, and the first two rows are $(1,0)$ and $(1,1)$.
	
	Inspection (or other techniques for solving linear equations) will show you that this is possible by
	\[\left(0 \begin{pmatrix} 1 \\ 2 \\ 4 \end{pmatrix} + \frac{1}{2} \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix} \ ,  \begin{pmatrix} 1 \\ 2 \\ 4 \end{pmatrix} - \frac{1}{2} \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix} \right) = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ \frac{1}{2} & \frac{7}{2} \end{pmatrix} = A. \]
	
	And so we see that 
	\begin{align*}
		A &= \begin{pmatrix} 1 \\ 2 \\ 4 \end{pmatrix} \begin{pmatrix} 0 & 1 \end{pmatrix} + \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix} \begin{pmatrix} \frac{1}{2} & -\frac{1}{2} \end{pmatrix} \\
		&= \begin{pmatrix} 1 & 2 \\ 2 & 2 \\ 4 & 1 \end{pmatrix}\begin{pmatrix} 0 & 1 \\ \frac{1}{2} & -\frac{1}{2}\end{pmatrix}.
	\end{align*}
	So we can take
	\[v = \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \qquad z = \begin{pmatrix} \frac{1}{2} \\ -\frac{1}{2}\end{pmatrix}. \]
\end{proof}


\begin{prob} If a subspace $S$ is contained in a subspace $V$, prove that $S^\perp$ contains $V^\perp$.
\end{prob}

\begin{proof}
	Suppose $w \in V^\perp$. Then for all $v \in V$, $w \cdot v=0$. Since $S$ is contained in $V$, this means that for all $s \in S$, we have $w \cdot s=0$. So by the definition of $S^\perp$, $w$ is in $S^\perp$. That is, for every vector $w \in V^\perp$, we have shown $w$ is also in $S^\perp$. This means $V^\perp \subset S^\perp$.
\end{proof}


\end{document}